{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "278677it [22:00, 211.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи выделения именованных сущностей я взяла датасет про одежду, обувь и украшения. Предварительно сделала токенизацию и лемматизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_texts = []\n",
    "for line in tqdm(open('reviews.json', 'r')):\n",
    "    tok_text = word_tokenize(json.loads(line)['reviewText'])\n",
    "    lemmas = [m.parse(word)[0].normal_form for word in tok_text if word.isalpha()]\n",
    "    lemm_texts.append(lemmas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три способа выделить именованные сущности:\n",
    "1. Составить правила. Достоинства: можем четко задать правила, и не получить лишние сущности. Недостаток метода состоит в том, что правила необходимо прописывать вручную, и это занимает определенное количество времени. Кроме того, название брендов и дескрипторов как правило достаточно разнообразные, и их будет достаточно сложно выделять с помощью правил.\n",
    "2. Классификационный метод. Можем выделить в тексте всех кандидатов (например все ИГ), выделить для них признаки и классифицировать по ним. Недостаток метода - нужны размеченные данные, которых у нас нет\n",
    "3. Составить список базовых дескрипторов и брендов и расширить его с помощью эмбеддингов. Недостаток - можем получить лишние сущности, если плохо подберем порог близости. Этот метод хорошо нам подходит, поскольку можно будет составить список базовых дескрипторов, и затем расширить его, покрыв все поле.\n",
    "\n",
    "Воспользуемся третьим методом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=lemm_texts, vector_size=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим список дескрипторов и с помощью Word2vec найдем для каждого 5 ближайших слов. Также составим список оценочных прилагательных, и для каждого возьмем по 10 соседей, чтобы выделять именно коллокации с оценкой: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr = ['skirt', 'dress', 'jeans', 'jacket', 'shirt', 'socks', 'leggings', 'underwear', 'sweater', 'top',\n",
    "        'boots', 'shoes', 'sneakers', 'necklace', 'earrings']\n",
    "all_descr = []\n",
    "for word in descr:\n",
    "    if word not in all_descr:\n",
    "        all_descr.append(word)\n",
    "    for sim_word in model.wv.most_similar(word, topn=5):\n",
    "        if sim_word[0] not in all_descr:\n",
    "            all_descr.append(sim_word[0])\n",
    "eval = ['beautiful', 'comfortable', 'stylish', 'nice', 'uncomfortable', 'strange', 'unattractive']\n",
    "all_eval = []\n",
    "for word in eval:\n",
    "    if word not in all_eval:\n",
    "        all_eval.append(word)\n",
    "    for sim_word in model.wv.most_similar(word, topn=10):\n",
    "        if sim_word[0] not in all_eval:\n",
    "            all_eval.append(sim_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выделим все биграммы с необходимыми словами\n",
    "bigrams = []\n",
    "for text in lemm_texts:\n",
    "    for ind, word in enumerate(text):\n",
    "        if word in all_descr:\n",
    "            if ind != 0:\n",
    "                bigrams.append((text[ind-1], word))\n",
    "            if ind != len(text) - 1:\n",
    "                bigrams.append((word, text[ind+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Найдем коллокации, для отчета будем использовать pmi\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_documents(lemm_texts)\n",
    "finder.apply_freq_filter(10)\n",
    "pmi_coll = finder.score_ngrams(bigram_measures.pmi)\n",
    "log_coll = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "chi_coll = finder.score_ngrams(bigram_measures.chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#берем коллокации, найденные с помощью pmi, выбираем среди них те, \n",
    "#которые относятся к нашим товарам, а также содержат оценочные прилагательные\n",
    "goods_coll = list(set(bigrams) & set([coll[0] for coll in pmi_coll]))\n",
    "final_coll = {}\n",
    "for coll in goods_coll:\n",
    "    descr_word = set(all_descr) & set(coll)\n",
    "    eval_word = set(all_eval) & set(coll)\n",
    "    if len(eval_word) != 0:\n",
    "        d_word = list(descr_word)[0]\n",
    "        if d_word not in final_coll:\n",
    "            final_coll[d_word] = []\n",
    "        final_coll[d_word].append(coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sandals\n",
      "[('nice', 'sandals'), ('pretty', 'sandals'), ('cute', 'sandals'), ('great', 'sandals'), ('comfortable', 'sandals'), ('comfy', 'sandals'), ('sandals', 'great'), ('good', 'sandals')]\n",
      "socks\n",
      "[('wonderful', 'socks'), ('good', 'socks'), ('nice', 'socks'), ('lightweight', 'socks'), ('socks', 'great'), ('tight', 'socks'), ('great', 'socks'), ('cute', 'socks'), ('decent', 'socks'), ('comfortable', 'socks'), ('socks', 'nice'), ('comfy', 'socks'), ('socks', 'good')]\n",
      "boxers\n",
      "[('great', 'boxers'), ('comfortable', 'boxers')]\n",
      "jeans\n",
      "[('tight', 'jeans'), ('comfy', 'jeans'), ('jeans', 'great'), ('good', 'jeans'), ('nice', 'jeans'), ('great', 'jeans'), ('cute', 'jeans'), ('jeans', 'nice'), ('jeans', 'good'), ('comfortable', 'jeans')]\n",
      "blouse\n",
      "[('lovely', 'blouse'), ('nice', 'blouse'), ('beautiful', 'blouse'), ('pretty', 'blouse'), ('cute', 'blouse'), ('great', 'blouse')]\n"
     ]
    }
   ],
   "source": [
    "for good in list(final_coll)[:5]:\n",
    "    print(good)\n",
    "    print(final_coll[good])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d14f91597336588c429f58aaac29838098a40a03688d6438025d1510ae4a0cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
